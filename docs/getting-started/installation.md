# Installation Guide

This guide walks you through setting up Ollama PDF RAG on your system.

## Prerequisites

Before installing, ensure you have:

| Requirement | Version | Notes |
|-------------|---------|-------|
| Python | 3.10+ | Required for LangChain 1.0 |
| Node.js | 18+ | For Next.js frontend |
| pnpm | 9+ | Package manager for Node.js |
| Ollama | Latest | Local LLM server |
| Git | Any | For cloning the repo |

## Step 1: Install Ollama

### macOS / Linux

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Windows

Download from [ollama.com/download](https://ollama.com/download/windows)

### Verify Installation

```bash
ollama --version
```

## Step 2: Pull Required Models

```bash
# Chat model (choose one or more)
ollama pull llama3.2          # Recommended - fast & capable
ollama pull qwen3:8b          # Great for thinking/reasoning
ollama pull mistral           # Alternative option

# Embedding model (required)
ollama pull nomic-embed-text
```

!!! tip "Model Selection"
    - **qwen3:8b** and **deepseek-r1** support "thinking mode" for chain-of-thought reasoning
    - **llama3.2** is fastest for general use
    - You can pull multiple models and switch between them in the UI

## Step 3: Clone the Repository

```bash
git clone https://github.com/tonykipkemboi/ollama_pdf_rag.git
cd ollama_pdf_rag
```

## Step 4: Python Environment Setup

### Using venv (Recommended)

```bash
# Create virtual environment
python -m venv .venv

# Activate it
# macOS/Linux:
source .venv/bin/activate

# Windows:
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Using uv (Faster Alternative)

```bash
# Install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create environment and install
uv venv
source .venv/bin/activate
uv pip install -r requirements.txt
```

## Step 5: Next.js Frontend Setup

```bash
cd web-ui

# Install dependencies
pnpm install

# Return to project root
cd ..
```

## Step 6: Verify Installation

### Start the Backend

```bash
python run_api.py
```

You should see:

```
INFO:     Uvicorn running on http://0.0.0.0:8001
INFO:     Application startup complete.
```

### Start the Frontend (New Terminal)

```bash
cd web-ui
pnpm dev
```

You should see:

```
â–² Next.js 16.0.10
- Local:        http://localhost:3000
```

### Test the App

1. Open [http://localhost:3000](http://localhost:3000)
2. You should see the PDF Chat interface
3. Upload a PDF using the ðŸ“Ž button
4. Select it with the checkbox
5. Ask a question!

## Directory Structure

After installation, your directory should look like:

```
ollama_pdf_rag/
â”œâ”€â”€ .venv/                 # Python virtual environment
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pdfs/             # Uploaded PDFs
â”‚   â”‚   â”œâ”€â”€ sample/       # Sample PDFs
â”‚   â”‚   â””â”€â”€ uploads/      # User uploads
â”‚   â”œâ”€â”€ vectors/          # ChromaDB vector storage
â”‚   â””â”€â”€ api.db            # SQLite metadata
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/              # FastAPI backend
â”‚   â”œâ”€â”€ app/              # Streamlit app
â”‚   â””â”€â”€ core/             # Core RAG logic
â”œâ”€â”€ web-ui/               # Next.js frontend
â”‚   â”œâ”€â”€ app/              # Next.js app router
â”‚   â”œâ”€â”€ components/       # React components
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ chat.db       # Chat history
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ run_api.py           # Start FastAPI server
```

## Troubleshooting

### ONNX DLL Error (Windows)

```
DLL load failed while importing onnx_copy2py_export
```

**Solution:**

1. Install [Microsoft Visual C++ Redistributable](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist)
2. Restart your computer
3. Or try:
   ```bash
   pip uninstall onnxruntime onnxruntime-gpu
   pip install onnxruntime
   ```

### Port Already in Use

```
Address already in use: 8001
```

**Solution:**

```bash
# Find process using the port
lsof -i :8001

# Kill it
kill -9 <PID>
```

### Model Not Found

```
Model 'llama3.2' not found
```

**Solution:**

```bash
# Pull the model
ollama pull llama3.2

# Verify it's available
ollama list
```

### pnpm Not Found

```bash
# Install pnpm globally
npm install -g pnpm

# Or using corepack (Node.js 16.10+)
corepack enable
corepack prepare pnpm@latest --activate
```

### SQLite Errors

If you see database errors, delete the database files and restart:

```bash
rm -f data/api.db
rm -f web-ui/data/chat.db
```

## Next Steps

- Follow the [Quick Start Guide](quickstart.md) to use the app
- Read about [PDF Processing](../user-guide/pdf-processing.md)
- Learn the [RAG Pipeline](../user-guide/rag-pipeline.md)
- Check the [API Reference](../api/document.md)
