{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ollama PDF RAG Documentation","text":"<p>Welcome to the documentation for Ollama PDF RAG, a powerful local RAG (Retrieval Augmented Generation) application that lets you chat with your PDF documents using Ollama and LangChain.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project provides both a Streamlit web interface and a Jupyter notebook for experimenting with PDF-based question answering using local language models. All processing happens locally on your machine, ensuring privacy and data security.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd12 Fully Local Processing: No data leaves your machine</li> <li>\ud83d\udcc4 PDF Processing: Intelligent chunking and processing of PDF documents</li> <li>\ud83e\udde0 Multi-query Retrieval: Better context understanding through multiple query generation</li> <li>\ud83c\udfaf Advanced RAG: Sophisticated implementation using LangChain</li> <li>\ud83d\udda5\ufe0f Clean Interface: User-friendly Streamlit interface</li> <li>\ud83d\udcd3 Experimentation: Jupyter notebook for testing and development</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>User Guide</li> <li>API Reference</li> <li>Contributing Guide</li> </ul>"},{"location":"#project-status","title":"Project Status","text":""},{"location":"#support","title":"Support","text":"<p>If you need help or want to contribute:</p> <ul> <li>\ud83d\udc1b Report a bug</li> <li>\ud83d\udca1 Request a feature</li> <li>\ud83e\udd1d Contribute to the project</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is open source and available under the MIT License. </p>"},{"location":"api/document/","title":"Document Processing API","text":"<p>This page documents the document processing components of Ollama PDF RAG.</p>"},{"location":"api/document/#documentprocessor","title":"DocumentProcessor","text":"<pre><code>class DocumentProcessor:\n    \"\"\"Handles PDF document loading and processing.\"\"\"\n\n    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n        \"\"\"Initialize document processor with chunking parameters.\"\"\"\n</code></pre>"},{"location":"api/document/#methods","title":"Methods","text":""},{"location":"api/document/#load_document","title":"load_document","text":"<pre><code>def load_document(self, file_path: str) -&gt; List[Document]:\n    \"\"\"Load a PDF document and return list of Document objects.\"\"\"\n</code></pre> <p>Parameters: - <code>file_path</code>: Path to the PDF file</p> <p>Returns: - List of Document objects</p>"},{"location":"api/document/#split_documents","title":"split_documents","text":"<pre><code>def split_documents(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Split documents into chunks with overlap.\"\"\"\n</code></pre> <p>Parameters: - <code>documents</code>: List of Document objects</p> <p>Returns: - List of chunked Document objects</p>"},{"location":"api/document/#process_pdf","title":"process_pdf","text":"<pre><code>def process_pdf(self, file_path: str) -&gt; List[Document]:\n    \"\"\"Load and process a PDF file.\"\"\"\n</code></pre> <p>Parameters: - <code>file_path</code>: Path to the PDF file</p> <p>Returns: - List of processed Document chunks</p>"},{"location":"api/document/#usage-example","title":"Usage Example","text":"<pre><code># Initialize processor\nprocessor = DocumentProcessor(chunk_size=1000, chunk_overlap=200)\n\n# Process a PDF file\ndocuments = processor.process_pdf(\"path/to/document.pdf\")\n\n# Access document content\nfor doc in documents:\n    print(doc.page_content)\n    print(doc.metadata)\n</code></pre>"},{"location":"api/document/#configuration","title":"Configuration","text":"<p>The document processor can be configured with:</p> <ul> <li><code>chunk_size</code>: Number of characters per chunk</li> <li><code>chunk_overlap</code>: Number of overlapping characters</li> <li><code>pdf_parser</code>: PDF parsing backend</li> <li><code>encoding</code>: Text encoding</li> </ul>"},{"location":"api/document/#error-handling","title":"Error Handling","text":"<p>The processor handles common errors:</p> <ul> <li>File not found</li> <li>Invalid PDF format</li> <li>Encoding issues</li> <li>Memory constraints </li> </ul>"},{"location":"api/embeddings/","title":"Embeddings API","text":"<p>This page documents the text embedding components used for semantic search.</p>"},{"location":"api/embeddings/#nomicembeddings","title":"NomicEmbeddings","text":"<pre><code>class NomicEmbeddings:\n    \"\"\"Manages text embeddings using Nomic's embedding model.\"\"\"\n\n    def __init__(self, model_name: str = \"nomic-embed-text\"):\n        \"\"\"Initialize embeddings with model name.\"\"\"\n</code></pre>"},{"location":"api/embeddings/#methods","title":"Methods","text":""},{"location":"api/embeddings/#embed_documents","title":"embed_documents","text":"<pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n    \"\"\"Generate embeddings for a list of texts.\"\"\"\n</code></pre> <p>Parameters: - <code>texts</code>: List of text strings</p> <p>Returns: - List of embedding vectors</p>"},{"location":"api/embeddings/#embed_query","title":"embed_query","text":"<pre><code>def embed_query(self, text: str) -&gt; List[float]:\n    \"\"\"Generate embedding for a single query text.\"\"\"\n</code></pre> <p>Parameters: - <code>text</code>: Query text</p> <p>Returns: - Embedding vector</p>"},{"location":"api/embeddings/#usage-example","title":"Usage Example","text":"<pre><code># Initialize embeddings\nembeddings = NomicEmbeddings()\n\n# Embed documents\ndocs = [\"First document\", \"Second document\"]\ndoc_embeddings = embeddings.embed_documents(docs)\n\n# Embed query\nquery = \"Sample query\"\nquery_embedding = embeddings.embed_query(query)\n</code></pre>"},{"location":"api/embeddings/#configuration","title":"Configuration","text":"<p>Configure embeddings with:</p> <ul> <li>Model selection</li> <li>Batch size</li> <li>Normalization</li> <li>Caching options</li> </ul>"},{"location":"api/embeddings/#performance","title":"Performance","text":"<p>Optimization options:</p> <ul> <li>Batch processing</li> <li>GPU acceleration</li> <li>Caching</li> <li>Dimensionality</li> </ul>"},{"location":"api/embeddings/#best-practices","title":"Best Practices","text":"<ol> <li>Text Preparation</li> <li>Clean input text</li> <li>Handle special characters</li> <li> <p>Normalize length</p> </li> <li> <p>Resource Management</p> </li> <li>Batch similar lengths</li> <li>Monitor memory usage</li> <li> <p>Cache frequent queries</p> </li> <li> <p>Quality Control</p> </li> <li>Validate embeddings</li> <li>Check dimensions</li> <li>Monitor similarity scores ``` </li> </ol>"},{"location":"api/llm/","title":"LLM Manager API","text":"<p>This page documents the Language Model management components.</p>"},{"location":"api/llm/#llmmanager","title":"LLMManager","text":"<pre><code>class LLMManager:\n    \"\"\"Manages Ollama language model interactions.\"\"\"\n\n    def __init__(self, model_name: str = \"llama2\"):\n        \"\"\"Initialize LLM manager with model name.\"\"\"\n</code></pre>"},{"location":"api/llm/#methods","title":"Methods","text":""},{"location":"api/llm/#list_models","title":"list_models","text":"<pre><code>def list_models() -&gt; List[str]:\n    \"\"\"List available Ollama models.\"\"\"\n</code></pre> <p>Returns: - List of model names</p>"},{"location":"api/llm/#get_model","title":"get_model","text":"<pre><code>def get_model(self, model_name: str) -&gt; LLM:\n    \"\"\"Get an instance of the specified model.\"\"\"\n</code></pre> <p>Parameters: - <code>model_name</code>: Name of the Ollama model</p> <p>Returns: - LLM instance</p>"},{"location":"api/llm/#generate","title":"generate","text":"<pre><code>def generate(self, prompt: str, **kwargs) -&gt; str:\n    \"\"\"Generate text using the current model.\"\"\"\n</code></pre> <p>Parameters: - <code>prompt</code>: Input text - <code>**kwargs</code>: Additional generation parameters</p> <p>Returns: - Generated text</p>"},{"location":"api/llm/#usage-example","title":"Usage Example","text":"<pre><code># Initialize manager\nmanager = LLMManager(model_name=\"llama2\")\n\n# List available models\nmodels = manager.list_models()\n\n# Generate text\nresponse = manager.generate(\n    prompt=\"Explain RAG in simple terms\",\n    temperature=0.7,\n    max_tokens=500\n)\n</code></pre>"},{"location":"api/llm/#model-parameters","title":"Model Parameters","text":"<p>Configure model behavior with:</p> <ul> <li><code>temperature</code>: Creativity (0.0-1.0)</li> <li><code>max_tokens</code>: Response length</li> <li><code>top_p</code>: Nucleus sampling</li> <li><code>frequency_penalty</code>: Repetition control</li> </ul>"},{"location":"api/llm/#error-handling","title":"Error Handling","text":"<p>The manager handles:</p> <ul> <li>Model loading errors</li> <li>Generation timeouts</li> <li>Resource constraints</li> <li>API communication issues</li> </ul>"},{"location":"api/llm/#best-practices","title":"Best Practices","text":"<ol> <li>Model Selection</li> <li>Match model to task</li> <li>Consider resource usage</li> <li> <p>Test performance</p> </li> <li> <p>Parameter Tuning</p> </li> <li>Adjust temperature</li> <li>Control response length</li> <li>Balance quality/speed ``` </li> </ol>"},{"location":"api/rag/","title":"RAG Pipeline API","text":"<p>This page documents the RAG (Retrieval Augmented Generation) pipeline components.</p>"},{"location":"api/rag/#ragpipeline","title":"RAGPipeline","text":"<pre><code>class RAGPipeline:\n    \"\"\"Manages the RAG pipeline for document question-answering.\"\"\"\n\n    def __init__(self, model_name: str, embeddings: Embeddings):\n        \"\"\"Initialize RAG pipeline with model and embeddings.\"\"\"\n</code></pre>"},{"location":"api/rag/#methods","title":"Methods","text":""},{"location":"api/rag/#create_vector_store","title":"create_vector_store","text":"<pre><code>def create_vector_store(self, documents: List[Document]) -&gt; VectorStore:\n    \"\"\"Create a vector store from documents.\"\"\"\n</code></pre> <p>Parameters: - <code>documents</code>: List of processed documents</p> <p>Returns: - Initialized vector store</p>"},{"location":"api/rag/#get_relevant_documents","title":"get_relevant_documents","text":"<pre><code>def get_relevant_documents(self, query: str) -&gt; List[Document]:\n    \"\"\"Retrieve relevant documents for a query.\"\"\"\n</code></pre> <p>Parameters: - <code>query</code>: User question - <code>k</code>: Number of documents to retrieve (default: 4)</p> <p>Returns: - List of relevant documents</p>"},{"location":"api/rag/#generate_response","title":"generate_response","text":"<pre><code>def generate_response(self, query: str, context: List[Document]) -&gt; str:\n    \"\"\"Generate response using LLM and context.\"\"\"\n</code></pre> <p>Parameters: - <code>query</code>: User question - <code>context</code>: Retrieved documents</p> <p>Returns: - Generated response</p>"},{"location":"api/rag/#usage-example","title":"Usage Example","text":"<pre><code># Initialize pipeline\npipeline = RAGPipeline(\n    model_name=\"llama2\",\n    embeddings=NonicEmbeddings()\n)\n\n# Process query\ndocs = pipeline.get_relevant_documents(\"What is RAG?\")\nresponse = pipeline.generate_response(\n    query=\"What is RAG?\",\n    context=docs\n)\n</code></pre>"},{"location":"api/rag/#configuration","title":"Configuration","text":"<p>The pipeline can be configured with:</p> <ul> <li>Model selection</li> <li>Embedding type</li> <li>Retrieval parameters</li> <li>Response templates</li> </ul>"},{"location":"api/rag/#performance-tuning","title":"Performance Tuning","text":"<p>Optimize the pipeline by adjusting:</p> <ul> <li>Number of retrieved documents</li> <li>Context window size</li> <li>Temperature setting</li> <li>Response length </li> </ul>"},{"location":"development/changelog/","title":"Changelog","text":"<p>The changelog is maintained in the root of the project: CHANGELOG.md</p>"},{"location":"development/changelog/#latest-release","title":"Latest Release","text":""},{"location":"development/changelog/#v210-2024-01-07","title":"[v2.1.0] - 2024-01-07","text":""},{"location":"development/changelog/#added","title":"Added","text":"<ul> <li>Comprehensive test suite with pytest</li> <li>GitHub Actions CI pipeline</li> <li>Pre-commit hooks for code quality</li> <li>Test coverage reporting</li> <li>Project restructuring with clean architecture</li> <li>New directory structure for better organization</li> <li>Sample PDFs in dedicated folder</li> </ul>"},{"location":"development/changelog/#changed","title":"Changed","text":"<ul> <li>Moved all source code to src/ directory</li> <li>Updated dependencies to latest compatible versions</li> <li>Improved README with testing documentation</li> <li>Added test status badge</li> <li>Reorganized PDF storage structure</li> </ul>"},{"location":"development/changelog/#fixed","title":"Fixed","text":"<ul> <li>Dependency conflicts with pydantic</li> <li>ONNX runtime compatibility issues</li> <li>Test coverage configuration</li> </ul> <p>View full changelog </p>"},{"location":"development/contributing/","title":"Contributing Guide","text":"<p>Thank you for considering contributing to Ollama PDF RAG! This document provides guidelines and instructions for contributing.</p>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project follows a Code of Conduct that all contributors are expected to adhere to. Please read CODE_OF_CONDUCT.md before contributing.</p>"},{"location":"development/contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"development/contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<ol> <li>Fork the repository</li> <li> <p>Clone your fork:    <pre><code>git clone https://github.com/YOUR_USERNAME/ollama_pdf_rag.git\ncd ollama_pdf_rag\n</code></pre></p> </li> <li> <p>Create a virtual environment:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: .\\venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install development dependencies:    <pre><code>pip install -r requirements.txt\npip install pre-commit pytest pytest-cov\n</code></pre></p> </li> <li> <p>Set up pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a new branch:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes</p> </li> <li> <p>Run tests:    <pre><code>pytest\n</code></pre></p> </li> <li> <p>Commit your changes:    <pre><code>git add .\ngit commit -m \"feat: Add new feature\"\n</code></pre></p> </li> <li> <p>Push to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a Pull Request</p> </li> </ol>"},{"location":"development/contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>We follow conventional commits. Format: <pre><code>type(scope): description\n\n[optional body]\n\n[optional footer]\n</code></pre></p> <p>Types: - feat: New feature - fix: Bug fix - docs: Documentation - style: Formatting - refactor: Code restructuring - test: Adding tests - chore: Maintenance</p>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=src\n\n# Run specific test file\npytest tests/test_document.py\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>Update documentation for any new features</li> <li>Add docstrings to new functions/classes</li> <li>Update README.md if needed</li> </ul>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Update documentation</li> <li>Add tests for new features</li> <li>Ensure all tests pass</li> <li>Update CHANGELOG.md</li> <li>Request review from maintainers</li> </ol>"},{"location":"development/contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Add docstrings (Google style)</li> <li>Keep functions focused and small</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Open an issue for bugs</li> <li>Discuss major changes in issues first</li> <li>Join our community discussions</li> </ul>"},{"location":"development/contributing/#release-process","title":"Release Process","text":"<ol> <li>Update version in relevant files</li> <li>Update CHANGELOG.md</li> <li>Create a new release on GitHub</li> <li>Tag the release</li> </ol>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License. </p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers testing practices and procedures for Ollama PDF RAG.</p>"},{"location":"development/testing/#overview","title":"Overview","text":"<p>We use pytest for testing and maintain high test coverage to ensure code quality. Tests are organized by component and include unit tests, integration tests, and end-to-end tests.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 test_document.py    # Document processing tests\n\u251c\u2500\u2500 test_models.py      # Model extraction tests\n\u2514\u2500\u2500 test_rag.py        # RAG pipeline tests\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npytest\n\n# Run tests with output\npytest -v\n\n# Run specific test file\npytest tests/test_document.py\n\n# Run specific test function\npytest tests/test_document.py::test_split_documents\n</code></pre>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Generate coverage report\npytest --cov=src\n\n# Generate detailed coverage report\npytest --cov=src --cov-report=term-missing\n\n# Generate HTML coverage report\npytest --cov=src --cov-report=html\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#test-file-structure","title":"Test File Structure","text":"<pre><code>\"\"\"Test module docstring.\"\"\"\nimport pytest\nfrom unittest.mock import Mock, patch\n\ndef test_function_name():\n    \"\"\"Test docstring explaining what is being tested.\"\"\"\n    # Arrange\n    input_data = ...\n\n    # Act\n    result = function_to_test(input_data)\n\n    # Assert\n    assert result == expected_output\n</code></pre>"},{"location":"development/testing/#using-fixtures","title":"Using Fixtures","text":"<pre><code>@pytest.fixture\ndef processor():\n    \"\"\"Create a DocumentProcessor instance.\"\"\"\n    return DocumentProcessor()\n\ndef test_with_fixture(processor):\n    \"\"\"Test using the fixture.\"\"\"\n    result = processor.process_document()\n    assert result is not None\n</code></pre>"},{"location":"development/testing/#mocking","title":"Mocking","text":"<pre><code>@patch('module.class.method')\ndef test_with_mock(mock_method):\n    \"\"\"Test using a mock.\"\"\"\n    mock_method.return_value = expected_value\n    result = function_that_uses_method()\n    assert result == expected_value\n</code></pre>"},{"location":"development/testing/#test-categories","title":"Test Categories","text":""},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":"<ul> <li>Test individual functions/methods</li> <li>Mock dependencies</li> <li>Fast execution</li> <li>High coverage</li> </ul>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":"<ul> <li>Test component interactions</li> <li>Minimal mocking</li> <li>Focus on integration points</li> </ul>"},{"location":"development/testing/#end-to-end-tests","title":"End-to-End Tests","text":"<ul> <li>Test complete workflows</li> <li>No mocking</li> <li>Slower execution</li> <li>Critical path coverage</li> </ul>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":"<ol> <li>Test Organization</li> <li>One test file per module</li> <li>Clear test names</li> <li> <p>Descriptive docstrings</p> </li> <li> <p>Test Independence</p> </li> <li>Tests should not depend on each other</li> <li>Clean up after tests</li> <li> <p>Use fixtures for setup/teardown</p> </li> <li> <p>Test Coverage</p> </li> <li>Aim for high coverage</li> <li>Focus on critical paths</li> <li> <p>Test edge cases</p> </li> <li> <p>Assertions</p> </li> <li>One assertion per test when possible</li> <li>Clear failure messages</li> <li>Test both positive and negative cases</li> </ol>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":"<p>Tests run automatically on: - Every push to main - Pull requests - Release tags</p> <p>GitHub Actions configuration: <pre><code>name: Python Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Run tests\n        run: pytest\n</code></pre></p>"},{"location":"development/testing/#common-issues","title":"Common Issues","text":""},{"location":"development/testing/#test-performance","title":"Test Performance","text":"<ul> <li>Use appropriate fixtures</li> <li>Mock expensive operations</li> <li>Parallelize test execution</li> </ul>"},{"location":"development/testing/#flaky-tests","title":"Flaky Tests","text":"<ul> <li>Avoid time-dependent tests</li> <li>Use stable test data</li> <li>Handle async operations properly</li> </ul>"},{"location":"development/testing/#coverage-gaps","title":"Coverage Gaps","text":"<ul> <li>Identify uncovered code</li> <li>Add missing test cases</li> <li>Focus on critical functionality</li> </ul>"},{"location":"development/testing/#resources","title":"Resources","text":"<ul> <li>pytest Documentation</li> <li>Coverage.py Documentation</li> <li>Python Testing Best Practices </li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will help you set up Ollama PDF RAG on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing Ollama PDF RAG, ensure you have:</p> <ol> <li>Python 3.9 or higher installed</li> <li>pip (Python package installer)</li> <li>git installed</li> <li>Ollama installed on your system</li> </ol>"},{"location":"getting-started/installation/#installing-ollama","title":"Installing Ollama","text":"<ol> <li>Visit Ollama's website to download and install the application</li> <li>After installation, pull the required models:    <pre><code>ollama pull llama3.2  # or your preferred model\nollama pull nomic-embed-text\n</code></pre></li> </ol>"},{"location":"getting-started/installation/#installing-ollama-pdf-rag","title":"Installing Ollama PDF RAG","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/tonykipkemboi/ollama_pdf_rag.git\ncd ollama_pdf_rag\n</code></pre></p> </li> <li> <p>Create and activate a virtual environment:    <pre><code># On macOS/Linux\npython -m venv venv\nsource venv/bin/activate\n\n# On Windows\npython -m venv venv\n.\\venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<ol> <li>Start Ollama in the background</li> <li>Run the application:    <pre><code>python run.py\n</code></pre></li> <li>Open your browser to <code>http://localhost:8501</code></li> </ol>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#onnx-dll-error","title":"ONNX DLL Error","text":"<p>If you see this error: <pre><code>DLL load failed while importing onnx_copy2py_export: a dynamic link Library (DLL) initialization routine failed.\n</code></pre></p> <p>Try these solutions:</p> <ol> <li>Install Microsoft Visual C++ Redistributable:</li> <li>Download both x64 and x86 versions from Microsoft's website</li> <li> <p>Restart your computer</p> </li> <li> <p>Or reinstall ONNX Runtime:    <pre><code>pip uninstall onnxruntime onnxruntime-gpu\npip install onnxruntime\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#cpu-only-systems","title":"CPU-Only Systems","text":"<p>For systems without a GPU:</p> <ol> <li> <p>Install CPU version of ONNX Runtime:    <pre><code>pip uninstall onnxruntime-gpu\npip install onnxruntime\n</code></pre></p> </li> <li> <p>Adjust chunk size if needed:</p> </li> <li>Reduce to 500-1000 for memory issues</li> <li>Increase overlap for better context</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quick Start Guide to begin using the application</li> <li>Read the User Guide for detailed usage instructions</li> <li>Check out the Contributing Guide if you want to help develop the project </li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>This guide will help you get started with Ollama PDF RAG quickly.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have: - Completed the installation - Started the Ollama service - Pulled the required models:   <pre><code>ollama pull llama3.2\nollama pull nomic-embed-text\n</code></pre></p>"},{"location":"getting-started/quickstart/#starting-the-application","title":"Starting the Application","text":"<ol> <li> <p>Activate your virtual environment:    <pre><code>source venv/bin/activate  # On Windows: .\\venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Start the application:    <pre><code>python run.py\n</code></pre></p> </li> <li> <p>Open your browser to <code>http://localhost:8501</code></p> </li> </ol>"},{"location":"getting-started/quickstart/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/quickstart/#1-upload-a-pdf","title":"1. Upload a PDF","text":"<ul> <li>Use the file uploader in the sidebar</li> <li>Or try the sample PDF provided</li> </ul>"},{"location":"getting-started/quickstart/#2-select-a-model","title":"2. Select a Model","text":"<ul> <li>Choose from your locally available Ollama models</li> <li>Default is <code>llama3.2</code></li> </ul>"},{"location":"getting-started/quickstart/#3-ask-questions","title":"3. Ask Questions","text":"<ul> <li>Type your question in the chat input</li> <li>Press Enter or click Send</li> <li>Wait for the response</li> </ul>"},{"location":"getting-started/quickstart/#4-adjust-display","title":"4. Adjust Display","text":"<ul> <li>Use the zoom slider to adjust PDF visibility</li> <li>PDF pages are displayed on the right</li> </ul>"},{"location":"getting-started/quickstart/#5-clean-up","title":"5. Clean Up","text":"<ul> <li>Use \"Delete Collection\" when switching documents</li> <li>This ensures clean context for new documents</li> </ul>"},{"location":"getting-started/quickstart/#example-usage","title":"Example Usage","text":"<p>Here's a simple example workflow:</p> <ol> <li>Upload a PDF about machine learning</li> <li>Ask questions like:</li> <li>\"What are the main topics covered in this document?\"</li> <li>\"Can you explain the concept of X mentioned on page Y?\"</li> <li>\"Summarize the key findings\"</li> </ol>"},{"location":"getting-started/quickstart/#tips","title":"Tips","text":"<ul> <li>Start with broad questions to understand document content</li> <li>Be specific when asking about particular sections</li> <li>Use follow-up questions for clarification</li> <li>Clear the context when switching documents</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the PDF Processing Guide for advanced usage</li> <li>Learn about the RAG Pipeline</li> <li>Explore the Chat Interface </li> </ul>"},{"location":"user-guide/chat-interface/","title":"Chat Interface","text":"<p>This guide explains how to use the Streamlit-based chat interface in Ollama PDF RAG.</p>"},{"location":"user-guide/chat-interface/#interface-overview","title":"Interface Overview","text":"<p>The interface consists of: - PDF viewer (right side) - Chat window (left side) - Model selection (sidebar) - File upload (sidebar)</p>"},{"location":"user-guide/chat-interface/#features","title":"Features","text":""},{"location":"user-guide/chat-interface/#pdf-upload","title":"PDF Upload","text":"<ul> <li>Drag and drop PDFs</li> <li>Multiple file support</li> <li>File size limits apply</li> </ul>"},{"location":"user-guide/chat-interface/#model-selection","title":"Model Selection","text":"<ul> <li>Choose Ollama models</li> <li>View model status</li> <li>Switch models anytime</li> </ul>"},{"location":"user-guide/chat-interface/#chat-window","title":"Chat Window","text":"<ul> <li>Real-time responses</li> <li>Message history</li> <li>Source citations</li> <li>Clear conversation</li> </ul>"},{"location":"user-guide/chat-interface/#pdf-viewer","title":"PDF Viewer","text":"<ul> <li>Page navigation</li> <li>Zoom controls</li> <li>Highlight relevant sections</li> </ul>"},{"location":"user-guide/chat-interface/#using-the-interface","title":"Using the Interface","text":"<ol> <li>Getting Started</li> <li>Upload your PDF</li> <li>Select a model</li> <li> <p>Start chatting</p> </li> <li> <p>Asking Questions</p> </li> <li>Type in chat box</li> <li>Press Enter/Send</li> <li> <p>Wait for response</p> </li> <li> <p>Managing Context</p> </li> <li>Clear chat history</li> <li>Upload new documents</li> <li>Switch models</li> </ol>"},{"location":"user-guide/chat-interface/#tips-and-tricks","title":"Tips and Tricks","text":"<ol> <li>Better Responses</li> <li>Ask specific questions</li> <li>Reference page numbers</li> <li> <p>One topic at a time</p> </li> <li> <p>Performance</p> </li> <li>Monitor memory usage</li> <li>Clear chat regularly</li> <li> <p>Optimize PDF size</p> </li> <li> <p>Troubleshooting</p> </li> <li>Check model status</li> <li>Verify PDF loading</li> <li>Monitor response time</li> </ol>"},{"location":"user-guide/chat-interface/#keyboard-shortcuts","title":"Keyboard Shortcuts","text":"<ul> <li><code>Enter</code>: Send message</li> <li><code>Shift + Enter</code>: New line</li> <li><code>Ctrl/Cmd + K</code>: Clear chat </li> </ul>"},{"location":"user-guide/pdf-processing/","title":"PDF Processing","text":"<p>This guide explains how the PDF processing works in Ollama PDF RAG.</p>"},{"location":"user-guide/pdf-processing/#document-loading","title":"Document Loading","text":"<p>The application uses LangChain's PDF loader to read and process PDF documents. Here's how it works:</p> <ol> <li>Upload a PDF through the Streamlit interface</li> <li>The PDF is loaded and parsed into text</li> <li>Text is split into manageable chunks</li> <li>Chunks are processed for better context retention</li> </ol>"},{"location":"user-guide/pdf-processing/#chunking-strategy","title":"Chunking Strategy","text":"<p>Documents are split using the following parameters:</p> <ul> <li>Chunk size: 1000 characters (configurable)</li> <li>Chunk overlap: 200 characters (configurable)</li> <li>Split by: Character</li> </ul> <p>This ensures: - Manageable chunk sizes for the model - Sufficient context overlap - Preservation of document structure</p>"},{"location":"user-guide/pdf-processing/#text-processing","title":"Text Processing","text":"<p>The text processing pipeline includes:</p> <ol> <li>Extraction: Converting PDF to raw text</li> <li>Cleaning: Removing artifacts and formatting</li> <li>Splitting: Creating overlapping chunks</li> <li>Indexing: Preparing for vector storage</li> </ol>"},{"location":"user-guide/pdf-processing/#configuration","title":"Configuration","text":"<p>You can adjust processing parameters in the application:</p> <pre><code>chunk_size = 1000  # Characters per chunk\nchunk_overlap = 200  # Overlap between chunks\n</code></pre>"},{"location":"user-guide/pdf-processing/#best-practices","title":"Best Practices","text":"<ol> <li>Document Quality</li> <li>Use searchable PDFs</li> <li>Ensure good scan quality</li> <li> <p>Check text extraction quality</p> </li> <li> <p>Chunk Size</p> </li> <li>Larger for detailed context</li> <li>Smaller for precise answers</li> <li> <p>Balance based on model capacity</p> </li> <li> <p>Memory Management</p> </li> <li>Monitor RAM usage</li> <li>Adjust chunk size if needed</li> <li>Clean up collections regularly </li> </ol>"},{"location":"user-guide/rag-pipeline/","title":"RAG Pipeline","text":"<p>This guide explains the Retrieval Augmented Generation (RAG) pipeline used in Ollama PDF RAG.</p>"},{"location":"user-guide/rag-pipeline/#overview","title":"Overview","text":"<p>The RAG pipeline combines document retrieval with language model generation to provide accurate, context-aware responses:</p> <ol> <li>Query Processing</li> <li>Document Retrieval</li> <li>Context Augmentation</li> <li>Response Generation</li> </ol>"},{"location":"user-guide/rag-pipeline/#components","title":"Components","text":""},{"location":"user-guide/rag-pipeline/#1-embeddings","title":"1. Embeddings","text":"<ul> <li>Uses Nomic's text embeddings</li> <li>Converts text chunks to vectors</li> <li>Enables semantic search</li> </ul>"},{"location":"user-guide/rag-pipeline/#2-vector-store","title":"2. Vector Store","text":"<ul> <li>ChromaDB for vector storage</li> <li>Efficient similarity search</li> <li>Persistent document storage</li> </ul>"},{"location":"user-guide/rag-pipeline/#3-retriever","title":"3. Retriever","text":"<ul> <li>Multi-query retrieval</li> <li>Semantic search</li> <li>Context window management</li> </ul>"},{"location":"user-guide/rag-pipeline/#4-language-model","title":"4. Language Model","text":"<ul> <li>Local Ollama models</li> <li>Context-aware responses</li> <li>Source attribution</li> </ul>"},{"location":"user-guide/rag-pipeline/#pipeline-flow","title":"Pipeline Flow","text":"<ol> <li>User Query</li> <li>Question is received</li> <li> <p>Query is processed</p> </li> <li> <p>Retrieval</p> </li> <li>Similar chunks found</li> <li> <p>Context assembled</p> </li> <li> <p>Generation</p> </li> <li>Context injected</li> <li>Response generated</li> <li>Sources tracked</li> </ol>"},{"location":"user-guide/rag-pipeline/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Chunk size tuning</li> <li>Embedding quality</li> <li>Model selection</li> <li>Memory management</li> </ul>"},{"location":"user-guide/rag-pipeline/#best-practices","title":"Best Practices","text":"<ol> <li>Query Formation</li> <li>Be specific</li> <li>One question at a time</li> <li> <p>Clear language</p> </li> <li> <p>Model Selection</p> </li> <li>Match to task</li> <li>Consider resources</li> <li> <p>Balance speed/quality</p> </li> <li> <p>Context Management</p> </li> <li>Monitor relevance</li> <li>Adjust retrieval</li> <li>Clean stale data </li> </ol>"}]}